import os
import sqlite3
from datetime import datetime

from loguru import logger

from ...utilities.db_utils import get_db_session
from ..database.models import ResearchLog

# Database path
# Use unified database in data directory
DATA_DIR = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "..", "..", "..", "data")
)
os.makedirs(DATA_DIR, exist_ok=True)
DB_PATH = os.path.join(DATA_DIR, "ldr.db")

# Legacy database paths (for migration)
LEGACY_RESEARCH_HISTORY_DB = os.path.abspath(
    os.path.join(
        os.path.dirname(__file__), "..", "..", "..", "research_history.db"
    )
)
LEGACY_DEEP_RESEARCH_DB = os.path.join(
    os.path.abspath(
        os.path.join(os.path.dirname(__file__), "..", "..", "..", "..", "data")
    ),
    "deep_research.db",
)


def get_db_connection():
    """
    Get a connection to the SQLite database.
    Allows for custom row factory if needed.
    """
    conn = sqlite3.connect(DB_PATH)
    return conn


def init_db():
    """Initialize the database with necessary tables."""
    conn = get_db_connection()
    cursor = conn.cursor()

    # Create a dedicated table for research logs
    cursor.execute(
        """
    CREATE TABLE IF NOT EXISTS research_logs (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        research_id INTEGER NOT NULL,
        timestamp TEXT NOT NULL,
        message TEXT NOT NULL,
        log_type TEXT NOT NULL,
        progress INTEGER,
        metadata TEXT,
        FOREIGN KEY (research_id) REFERENCES research_history (id) ON DELETE CASCADE
    )
    """
    )

    # Create a dedicated table for research resources
    cursor.execute(
        """
    CREATE TABLE IF NOT EXISTS research_resources (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        research_id INTEGER NOT NULL,
        title TEXT,
        url TEXT,
        content_preview TEXT,
        source_type TEXT,
        metadata TEXT,
        created_at TEXT NOT NULL,
        FOREIGN KEY (research_id) REFERENCES research_history (id) ON DELETE CASCADE
    )
    """
    )

    # Check if the duration_seconds column exists, add it if missing
    cursor.execute("PRAGMA table_info(research_history)")
    columns = [column[1] for column in cursor.fetchall()]

    if "duration_seconds" not in columns:
        logger.info(
            "Adding missing 'duration_seconds' column to research_history table"
        )
        cursor.execute(
            "ALTER TABLE research_history ADD COLUMN duration_seconds INTEGER"
        )

    # Check if the progress column exists, add it if missing
    if "progress" not in columns:
        logger.info(
            "Adding missing 'progress' column to research_history table"
        )
        cursor.execute(
            "ALTER TABLE research_history ADD COLUMN progress INTEGER"
        )

    # Check if the title column exists, add it if missing
    if "title" not in columns:
        logger.info("Adding missing 'title' column to research_history table")
        cursor.execute("ALTER TABLE research_history ADD COLUMN title TEXT")

    # Check if the metadata column exists, and rename it to "research_meta"
    # if it does.
    if "metadata" in columns:
        logger.info("Renaming 'metadata' column to 'research_meta'")
        cursor.execute(
            "ALTER TABLE research_history RENAME COLUMN metadata TO research_meta"
        )

    # Enable foreign key support
    cursor.execute("PRAGMA foreign_keys = ON")

    conn.commit()
    conn.close()


def calculate_duration(created_at_str, completed_at_str=None):
    """
    Calculate duration in seconds between created_at timestamp and completed_at or now.
    Handles various timestamp formats and returns None if calculation fails.

    Args:
        created_at_str: The start timestamp
        completed_at_str: Optional end timestamp, defaults to current time if None

    Returns:
        Duration in seconds or None if calculation fails
    """
    if not created_at_str:
        return None

    end_time = None
    if completed_at_str:
        # Use completed_at time if provided
        try:
            if "T" in completed_at_str:  # ISO format with T separator
                end_time = datetime.fromisoformat(completed_at_str)
            else:  # Older format without T
                # Try different formats
                try:
                    end_time = datetime.strptime(
                        completed_at_str, "%Y-%m-%d %H:%M:%S.%f"
                    )
                except ValueError:
                    try:
                        end_time = datetime.strptime(
                            completed_at_str, "%Y-%m-%d %H:%M:%S"
                        )
                    except ValueError:
                        # Last resort fallback
                        end_time = datetime.fromisoformat(
                            completed_at_str.replace(" ", "T")
                        )
        except Exception:
            logger.exception("Error parsing completed_at timestamp")
            try:
                from dateutil import parser

                end_time = parser.parse(completed_at_str)
            except Exception:
                logger.exception(
                    f"Fallback parsing also failed for completed_at: {completed_at_str}"
                )
                # Fall back to current time
                end_time = datetime.utcnow()
    else:
        # Use current time if no completed_at provided
        end_time = datetime.utcnow()

    start_time = None
    try:
        # Proper parsing of ISO format
        if "T" in created_at_str:  # ISO format with T separator
            start_time = datetime.fromisoformat(created_at_str)
        else:  # Older format without T
            # Try different formats
            try:
                start_time = datetime.strptime(
                    created_at_str, "%Y-%m-%d %H:%M:%S.%f"
                )
            except ValueError:
                try:
                    start_time = datetime.strptime(
                        created_at_str, "%Y-%m-%d %H:%M:%S"
                    )
                except ValueError:
                    # Last resort fallback
                    start_time = datetime.fromisoformat(
                        created_at_str.replace(" ", "T")
                    )
    except Exception:
        logger.exception("Error parsing created_at timestamp")
        # Fallback method if parsing fails
        try:
            from dateutil import parser

            start_time = parser.parse(created_at_str)
        except Exception:
            logger.exception(
                f"Fallback parsing also failed for created_at: {created_at_str}"
            )
            return None

    # Calculate duration if both timestamps are valid
    if start_time and end_time:
        try:
            return int((end_time - start_time).total_seconds())
        except Exception:
            logger.exception("Error calculating duration")

    return None


def get_logs_for_research(research_id):
    """
    Retrieve all logs for a specific research ID

    Args:
        research_id: ID of the research

    Returns:
        List of log entries as dictionaries
    """
    try:
        session = get_db_session()
        log_results = (
            session.query(ResearchLog)
            .filter(ResearchLog.research_id == research_id)
            .order_by(ResearchLog.timestamp.asc())
            .all()
        )

        logs = []
        for result in log_results:
            # Convert entry for frontend consumption
            formatted_entry = {
                "time": result.timestamp,
                "message": result.message,
                "type": result.level,
                "module": result.module,
                "line_no": result.line_no,
            }
            logs.append(formatted_entry)

        return logs
    except Exception:
        logger.exception("Error retrieving logs from database")
        return []


@logger.catch
def get_total_logs_for_research(research_id):
    """
    Returns the total number of logs for a given `research_id`.

    Args:
        research_id (int): The ID of the research.

    Returns:
        int: Total number of logs for the specified research ID.
    """
    session = get_db_session()
    total_logs = (
        session.query(ResearchLog)
        .filter(ResearchLog.research_id == research_id)
        .count()
    )
    return total_logs
