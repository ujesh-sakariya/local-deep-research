"""
API module for Local Deep Research.
Provides programmatic access to search and research capabilities.
"""

from typing import Dict, List, Optional, Union, Any, Callable
import logging
import os
import traceback
import toml
from ..search_system import AdvancedSearchSystem
from ..report_generator import IntegratedReportGenerator
from ..config import get_llm, get_search, settings
from ..utilties.search_utilities import remove_think_tags

logger = logging.getLogger(__name__)

def quick_summary(
    query: str,
    search_tool: Optional[str] = None,
    iterations: int = 1,
    questions_per_iteration: int = 1,
    max_results: int = 20,
    max_filtered_results: int = 5,
    region: str = "us",
    time_period: str = "y",
    safe_search: bool = True,
    temperature: float = 0.7,
    progress_callback: Optional[Callable] = None,
) -> Dict[str, Any]:
    """
    Generate a quick research summary for a given query.
    
    Args:
        query: The research query to analyze
        search_tool: Search engine to use (auto, wikipedia, arxiv, etc.). If None, uses default
        iterations: Number of research cycles to perform
        questions_per_iteration: Number of questions to generate per cycle
        max_results: Maximum number of search results to consider
        max_filtered_results: Maximum results after relevance filtering
        region: Search region/locale
        time_period: Time period for search results (d=day, w=week, m=month, y=year)
        safe_search: Whether to enable safe search
        temperature: LLM temperature for generation
        progress_callback: Optional callback function to receive progress updates
        
    Returns:
        Dictionary containing the research results with keys:
        - 'summary': The generated summary text
        - 'findings': List of detailed findings from each search
        - 'iterations': Number of iterations performed
        - 'questions': Questions generated during research
    """
    logger.info(f"Generating quick summary for query: {query}")
    

    # Get language model with custom temperature
    llm = get_llm(temperature=temperature)
    
    # Create search system with custom parameters
    system = AdvancedSearchSystem()
    
    # Override default settings with user-provided values
    system.max_iterations = iterations 
    system.questions_per_iteration = questions_per_iteration
    system.model = llm  # Ensure the model is directly attached to the system
    
    # Set the search engine if specified
    if search_tool:
        search_engine = get_search(search_tool)
        if search_engine:
            system.search = search_engine
        else:
            logger.warning(f"Could not create search engine '{search_tool}', using default.")
    
    # Set progress callback if provided
    if progress_callback:
        system.set_progress_callback(progress_callback)
    
    # Perform the search and analysis
    results = system.analyze_topic(query)
    
    # Extract the summary from the current knowledge
    if results and "current_knowledge" in results:
        summary = results["current_knowledge"]
    else:
        summary = "Unable to generate summary for the query."
    
    # Prepare the return value
    return {
        "summary": summary,
        "findings": results.get("findings", []),
        "iterations": results.get("iterations", 0),
        "questions": results.get("questions", {}),
        "formatted_findings": results.get("formatted_findings", ""),
        "sources": results.get("all_links_of_system", [])
    }


def generate_report(
    query: str,
    search_tool: Optional[str] = None,
    iterations: int = 2,
    questions_per_iteration: int = 2,
    searches_per_section: int = 2,
    max_results: int = 50,
    max_filtered_results: int = 5,
    region: str = "us",
    time_period: str = "y",
    safe_search: bool = True,
    temperature: float = 0.7,
    output_file: Optional[str] = None,
    progress_callback: Optional[Callable] = None,
) -> Dict[str, Any]:
    """
    Generate a comprehensive, structured research report for a given query.
    
    Args:
        query: The research query to analyze
        search_tool: Search engine to use (auto, wikipedia, arxiv, etc.). If None, uses default
        iterations: Number of research cycles to perform
        questions_per_iteration: Number of questions to generate per cycle
        searches_per_section: Number of searches to perform per report section
        max_results: Maximum number of search results to consider
        max_filtered_results: Maximum results after relevance filtering
        region: Search region/locale
        time_period: Time period for search results (d=day, w=week, m=month, y=year)
        safe_search: Whether to enable safe search
        temperature: LLM temperature for generation
        output_file: Optional path to save report markdown file
        progress_callback: Optional callback function to receive progress updates
        
    Returns:
        Dictionary containing the research report with keys:
        - 'content': The full report content in markdown format
        - 'metadata': Report metadata including generated timestamp and query
    """
    logger.info(f"Generating comprehensive research report for query: {query}")
    

    # Get language model with custom temperature
    llm = get_llm(temperature=temperature)
    
    # Create search system with custom parameters
    system = AdvancedSearchSystem()
    
    # Override default settings with user-provided values
    system.max_iterations = iterations
    system.questions_per_iteration = questions_per_iteration
    system.model = llm  # Ensure the model is directly attached to the system
    
    # Set the search engine if specified
    if search_tool:
        search_engine = get_search(
            search_tool,
            llm_instance=llm,
            max_results=max_results,
            max_filtered_results=max_filtered_results,
            region=region,
            time_period=time_period,
            safe_search=safe_search
        )
        if search_engine:
            system.search = search_engine
        else:
            logger.warning(f"Could not create search engine '{search_tool}', using default.")
    
    # Set progress callback if provided
    if progress_callback:
        system.set_progress_callback(progress_callback)
    
    # Perform the initial research
    initial_findings = system.analyze_topic(query)
    
    # Generate the structured report
    report_generator = IntegratedReportGenerator(searches_per_section=searches_per_section)
    report_generator.model = llm  # Ensure the model is set on the report generator too
    report = report_generator.generate_report(initial_findings, query)
    
    # Save report to file if path is provided
    if output_file and report and "content" in report:
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(report["content"])
            logger.info(f"Report saved to {output_file}")
            report["file_path"] = output_file
    return report



def analyze_documents(
    query: str,
    collection_name: str,
    max_results: int = 10,
    temperature: float = 0.7,
    force_reindex: bool = False,
    output_file: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Search and analyze documents in a specific local collection.
    
    Args:
        query: The search query
        collection_name: Name of the local document collection to search
        max_results: Maximum number of results to return
        temperature: LLM temperature for summary generation
        force_reindex: Whether to force reindexing the collection
        output_file: Optional path to save analysis results to a file
        
    Returns:
        Dictionary containing:
        - 'summary': Summary of the findings
        - 'documents': List of matching documents with content and metadata
    """
    logger.info(f"Analyzing documents in collection '{collection_name}' for query: {query}")
    

    # Get language model with custom temperature
    llm = get_llm(temperature=temperature)
    
    # Get search engine for the specified collection
    search = get_search(collection_name, llm_instance=llm)
    
    if not search:
        return {
            "summary": f"Error: Collection '{collection_name}' not found or not properly configured.",
            "documents": []
        }
    
    # Set max results
    search.max_results = max_results
    
    # Force reindex if requested
    if force_reindex and hasattr(search, 'embedding_manager'):
            for folder_path in search.folder_paths:
                search.embedding_manager.index_folder(folder_path, force_reindex=True)

    # Perform the search
    results = search.run(query)
    
    if not results:
        return {
            "summary": f"No documents found in collection '{collection_name}' for query: '{query}'",
            "documents": []
        }
    
    # Get LLM to generate a summary of the results

    docs_text = "\n\n".join([f"Document {i+1}: {doc.get('content', doc.get('snippet', ''))[:1000]}" 
                            for i, doc in enumerate(results[:5])])  # Limit to first 5 docs and 1000 chars each
    
    summary_prompt = f"""Analyze these document excerpts related to the query: "{query}"
    
    {docs_text}
    
    Provide a concise summary of the key information found in these documents related to the query.
    """
    
    summary_response = llm.invoke(summary_prompt)
    if hasattr(summary_response, 'content'):
        summary = remove_think_tags(summary_response.content)
    else:
        summary = str(summary_response)

    # Create result dictionary
    analysis_result = {
        "summary": summary,
        "documents": results,
        "collection": collection_name,
        "document_count": len(results)
    }
    
    # Save to file if requested
    if output_file:
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(f"# Document Analysis: {query}\n\n")
                f.write(f"## Summary\n\n{summary}\n\n")
                f.write(f"## Documents Found: {len(results)}\n\n")
                
                for i, doc in enumerate(results):
                    f.write(f"### Document {i+1}: {doc.get('title', 'Untitled')}\n\n")
                    f.write(f"**Source:** {doc.get('link', 'Unknown')}\n\n")
                    f.write(f"**Content:**\n\n{doc.get('content', doc.get('snippet', 'No content available'))[:1000]}...\n\n")
                    f.write("---\n\n")
                    
            analysis_result["file_path"] = output_file
            logger.info(f"Analysis saved to {output_file}")

    return analysis_result

def get_available_search_engines() -> Dict[str, str]:
    """
    Get a dictionary of available search engines.
    
    Returns:
        Dictionary mapping engine names to descriptions
    """

    from ..web_search_engines.search_engine_factory import get_available_engines
    engines = get_available_engines()
    
    # Add some descriptions for common engines
    descriptions = {
        "auto": "Automatic selection based on query type",
        "wikipedia": "Wikipedia articles and general knowledge",
        "arxiv": "Scientific papers and research",
        "pubmed": "Medical and biomedical literature",
        "semantic_scholar": "Academic papers across all fields",
        "github": "Code repositories and technical documentation",
        "local_all": "All local document collections"
    }
    
    return {engine: descriptions.get(engine, "Search engine") for engine in engines}


def get_available_collections() -> Dict[str, Dict[str, Any]]:
    """
    Get a dictionary of available local document collections.
    
    Returns:
        Dictionary mapping collection names to their configuration
    """


    from ..config import LOCAL_COLLECTIONS_FILE
    
    if os.path.exists(LOCAL_COLLECTIONS_FILE):
            collections = toml.load(LOCAL_COLLECTIONS_FILE)
            return collections

    return {}
