"""
Local Deep Research Configuration Guide

This configuration file controls the behavior of the research system.

MAIN SETTINGS:
- search_tool (str): Choose which search engine to use
  - "auto": Intelligent selection based on query content (recommended)
  - "local_all": Search only local document collections
  - "wikipedia": General knowledge and facts
  - "arxiv": Scientific papers and research
  - "duckduckgo": General web search (no API key needed)
  - "serp": Google search via SerpAPI (requires API key)
  - "google_pse": Google Programmable Search Engine for custom search experiences (requires API key)
  - "guardian": News articles (requires API key)
  - Any collection name from your local_collections.py

- DEFAULT_MODEL (str): LLM to use for analysis
  - "mistral": Default local model via Ollama
  - "deepseek-r1:14b": Alternative larger model
  - "claude-3-5-sonnet-latest": Claude model (requires API key)
  - "gpt-4o": OpenAI model (requires API key)
  - "mpt-7b": MPT model via VLLM

API KEYS:
For search engines requiring API keys, set these in your .env file:
- SERP_API_KEY: For Google search via SerpAPI
- GUARDIAN_API_KEY: For The Guardian news search
- GOOGLE_PSE_API_KEY: For Google Programmable Search Engine
- GOOGLE_PSE_ENGINE_ID: Search Engine ID from Google PSE Control Panel

RESEARCH SETTINGS:
- SEARCH_ITERATIONS (int): Number of research cycles
- QUESTIONS_PER_ITERATION (int): Questions per cycle
- SEARCHES_PER_SECTION (int): Searches per report section
- MAX_SEARCH_RESULTS (int): Results per search query
- MAX_FILTERED_RESULTS (int): Results after relevance filtering

For full documentation, see the README.md
"""

from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_community.llms import VLLM  # Added VLLM import
from dotenv import load_dotenv
from utilties.enums import KnowledgeAccumulationApproach

import os
import re
from typing import Dict, List, Any, Optional, Callable

# Load environment variables
load_dotenv()
# Choose search tool: "serp" or "duckduckgo" (serp requires API key)
search_tool = "searxng" # Change this variable to switch between search tools; for only local search "local-all"

KNOWLEDGE_ACCUMULATION = KnowledgeAccumulationApproach.ITERATION # None doesnt work with detailed report. It basically means the questions are seperate on the topic.
KNOWLEDGE_ACCUMULATION_CONTEXT_LIMIT = 2000000

# LLM Configuration
OPENAIENDPOINT=False # True + URL + Model Name
OPENROUTER_BASE_URL= "https://openrouter.ai/api/v1"

DEFAULT_MODEL = "mistral"  # try to use the largest model that fits into your GPU memory
DEFAULT_TEMPERATURE = 0.7
MAX_TOKENS = 30000

# VLLM Configuration
VLLM_MAX_NEW_TOKENS = 128
VLLM_TOP_K = 10
VLLM_TOP_P = 0.95
VLLM_TEMPERATURE = 0.8

# Search System Settings
SEARCH_ITERATIONS = 2
QUESTIONS_PER_ITERATION = 2

# Report settings
SEARCHES_PER_SECTION = 2
#CONTEXT_CUT_OFF = 10000

# citation handler
ENABLE_FACT_CHECKING = False  # comes with pros and cons. Maybe works better with larger LLMs?

# URL Quality Check (applies to both DDG and SerpAPI)
QUALITY_CHECK_DDG_URLS = True  # Keep True for better quality results.

SEARCH_SNIPPETS_ONLY = False
SKIP_RELEVANCE_FILTER = False 

# Search Configuration (applies to both DDG and SerpAPI)
MAX_SEARCH_RESULTS = 50  
MAX_FILTERED_RESULTS = 5
SEARCH_REGION = "us"
TIME_PERIOD = "y"
SAFE_SEARCH = True
SEARCH_LANGUAGE = "English"

# Output Configuration
OUTPUT_DIR = "research_outputs"

# SearXNG Configuration
SEARXNG_INSTANCE = "http://localhost:8080"
SEARXNG_DELAY = 2.0

# Make OpenAI integration optional with lazy loading
def is_openai_available():
    """Check if OpenAI is available without importing it at module level"""
    try:
        # Only import when checking
        import os
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            return False
            
        # Only try to import if we have an API key
        try:
            from langchain_openai import ChatOpenAI
            return True
        except ImportError:
            return False
    except:
        return False

# Models map
MODELS = {
    # Local models
    "ollama-llama3": "Local Llama3 model (requires Ollama)",
    "ollama-mixtral": "Local Mixtral model (requires Ollama)",
    "ollama-mistral": "Local Mistral model (requires Ollama)",
    
    # API-based models
    "gpt-4o": "OpenAI model (requires API key)",
    "gpt-3.5-turbo": "OpenAI model (requires API key)",
}

# Available model providers
MODEL_PROVIDERS = ["local", "openai"]

def get_model(model_name: str, **kwargs):
    """Get model instance based on the model name"""
    # Common parameters for all models
    common_params = {
        "temperature": kwargs.get("temperature", 0.0),
        "verbose": kwargs.get("verbose", False),
    }
    
    # Ollama models (local)
    if model_name.startswith("ollama-"):
        try:
            from langchain_community.llms import Ollama
            base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
            return Ollama(model=model_name.replace("ollama-", ""), base_url=base_url, **common_params)
        except ImportError:
            raise ValueError("Langchain Ollama integration not available. Please install it with: pip install langchain-community")
    
    # OpenAI models
    elif model_name in ["gpt-4o", "gpt-3.5-turbo"]:
        if not is_openai_available():
            raise ValueError("OpenAI integration not available. Please install langchain-openai and set OPENAI_API_KEY environment variable.")
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables")
        return ChatOpenAI(model=model_name, api_key=api_key, **common_params)
    
    else:
        raise ValueError(f"Unsupported model: {model_name}")

def get_available_models() -> Dict[str, str]:
    """Return available models based on installed packages and environment variables"""
    available_models = {}
    
    # Check Ollama models
    try:
        import langchain_community
        # Check if Ollama is running
        ollama_running = False
        try:
            import requests
            base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
            requests.get(f"{base_url}/api/tags", timeout=2)
            ollama_running = True
        except:
            pass
            
        if ollama_running:
            for model in [k for k in MODELS.keys() if k.startswith("ollama-")]:
                available_models[model] = MODELS[model]
    except ImportError:
        pass
    
    # Check OpenAI models
    if is_openai_available():
        for model in ["gpt-4o", "gpt-3.5-turbo"]:
            available_models[model] = MODELS[model]
    
    return available_models

def get_llm(model_name=DEFAULT_MODEL, temperature=DEFAULT_TEMPERATURE):
    """Get LLM instance - easy to switch between models"""
    common_params = {
        "temperature": temperature,
        "max_tokens": MAX_TOKENS,
    }

    if "claude" in model_name:
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY not found in environment variables")
        return ChatAnthropic(
            model=model_name, anthropic_api_key=api_key, **common_params
        )
    elif OPENAIENDPOINT:
        api_key = os.getenv("OPENAI_ENDPOINT_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables")
        return ChatOpenAI(model=model_name, api_key=api_key,openai_api_base=OPENROUTER_BASE_URL, **common_params)
       
    elif "gpt" in model_name:
        if not is_openai_available():
            raise ValueError("OpenAI integration not available or API key not set. Please install langchain-openai and set OPENAI_API_KEY.")
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables")
        return ChatOpenAI(model=model_name, api_key=api_key, **common_params)
        
    elif model_name == "mpt-7b":
        # VLLM configuration for the MPT model
        return VLLM(
            model="mosaicml/mpt-7b",
            trust_remote_code=True,  # mandatory for hf models
            max_new_tokens=VLLM_MAX_NEW_TOKENS,
            top_k=VLLM_TOP_K,
            top_p=VLLM_TOP_P,
            temperature=VLLM_TEMPERATURE,
        )

    else:
        base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        return ChatOllama(model=model_name, base_url=base_url, **common_params)


def get_search():
    """Get search tool instance based on config settings"""
    # Import here to avoid circular import
    from web_search_engines.search_engine_factory import get_search as factory_get_search
    
    print(f"Creating search engine with tool: {search_tool}")
    engine = factory_get_search(
        search_tool=search_tool,
        llm_instance=get_llm(),
        max_results=MAX_SEARCH_RESULTS,
        region=SEARCH_REGION,
        time_period=TIME_PERIOD,
        safe_search=SAFE_SEARCH,
        search_snippets_only=SEARCH_SNIPPETS_ONLY,
        search_language=SEARCH_LANGUAGE,
        max_filtered_results=MAX_FILTERED_RESULTS 
    )
    
    if engine is None:
        print(f"Failed to create search engine with tool: {search_tool}")
    
    return engine
